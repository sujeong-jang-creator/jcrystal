pipeline 구성시 적용한 실습


1. covid-19.csv 파일의 데이터 사용
    1단계 : logstash에도 없고 es에도 없던 데이터가 인식
        filebeat이 한번에 저장시도 해서 성공

    2단계 :
        이미 존재하는 file의 마직막 부분에 새로운 데이터 18개 정보 추가 저장

        filebeat이 추가된 데이터만 새로 저장 
        기존에 이미 있는것까지는 인식 하고 있음이 입증

        2단계 성공

    3단계
        2단계에서 추가한 데이터가 쓰레기 입니다 라고 하면서 18개를 삭제했음
        filebeats은 새로 logstash에 반영 안했음
        왜? 이미 18개 삭제 이전의 데이터는 logstash에 저장되어 있기 때문

        결론 : 추가하는 데이터 새로 저장은 가능
        이미 존재하는 데이터값은 변경하지 않는한 그대로 유지     


    참고
        기존 데이터 수정할 경우 
        가장 안정적이니 것은 백업된 모든 데이터를 다시 재저장하는 것을 더 선호
        100% 보장 불가이기 때문에
        이미 적재한 데이터를 수정시에는 ES 자체적으로 수정 권장
        filebeats 통해서 실시간 가변적인 새로운 데이터 적재(저장) 시도는 OK
        이미 저장시킨 데이터 수정 시도의 action은 비추     


2. pipeline 구성시 필수
    * 실습 내용
    1. 자동화
    2. 실시간 데이터를 크롤링해서 매일매일 파일로 생성
        - 코드 제공
    3. 매일매일 파일이 다른 파일명으로 구성시  
        - file명 권장 포멧 : 날짜를 가급적 포함 
            *_날짜...*
    4. 언제 크롤링 수행할지 자동화 
        - covid19_crawling.py 파일을 정해진 시간에 자동 실행되게 작업 

    5. ? 미션
        - 소스 분석 & 일정 시간에 자동으로 covid19_crawling.py 파일 실행되게 개발
    



* project 
    python - flask : 웹 브라우저로 요청/응답
           - rest api
           - RDMBS & Elastic Stack

    html / css / java script(비동기) 
    ..
    github 활용권장  - commit 흔적

    * 발표 : 21일 3시
    23일 : linux 기본 명령어 + aws 학습 (Elastic Stack + 작품)

    aws 작품을 배포

           
