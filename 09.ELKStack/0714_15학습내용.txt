pipeline 구성시 적용한 실습


1. covid-19.csv 파일의 데이터 사용
    1단계 : logstash에도 없고 es에도 없던 데이터가 인식
        filebeat이 한번에 저장시도 해서 성공

    2단계 :
        이미 존재하는 file의 마직막 부분에 새로운 데이터 18개 정보 추가 저장

        filebeat이 추가된 데이터만 새로 저장 
        기존에 이미 있는것까지는 인식 하고 있음이 입증

        2단계 성공

    3단계
        2단계에서 추가한 데이터가 쓰레기 입니다 라고 하면서 18개를 삭제했음
        filebeats은 새로 logstash에 반영 안했음
        왜? 이미 18개 삭제 이전의 데이터는 logstash에 저장되어 있기 때문

        결론 : 추가하는 데이터 새로 저장은 가능
        이미 존재하는 데이터값은 변경하지 않는한 그대로 유지     


    참고
        기존 데이터 수정할 경우 
        가장 안정적이니 것은 백업된 모든 데이터를 다시 재저장하는 것을 더 선호
        100% 보장 불가이기 때문에
        이미 적재한 데이터를 수정시에는 ES 자체적으로 수정 권장
        filebeats 통해서 실시간 가변적인 새로운 데이터 적재(저장) 시도는 OK
        이미 저장시킨 데이터 수정 시도의 action은 비추     


2. pipeline 구성시 필수
    * 실습 내용
    1. 자동화
    2. 실시간 데이터를 크롤링해서 매일매일 파일로 생성
        - 코드 제공
    3. 매일매일 파일이 다른 파일명으로 구성시  
        - file명 권장 포멧 : 날짜를 가급적 포함 
            *_날짜...*
    4. 언제 크롤링 수행할지 자동화 
        - covid19_crawling.py 파일을 정해진 시간에 자동 실행되게 작업 

    5. 미션
        - https://drive.google.com/drive/folders/1RA0u0HP4poABPMcrVQIon5WyNCIoz0NZ
        - 소스 분석 & 일정 시간에 자동으로 covid19_crawling.py 파일 실행되게 개발
        1. os
            설정방식이 다양
            1. window - os 자체적인 스케쥴러 등록 방식
            2. linux - 크론 자체의 설정을 주로 사용
            3. 언어 - 대부분 개발 언어가 이런 기능의 모듈(api 제시)
                코드로 순수하게 운영체제와 무관하게 개발 하는 방식

        2. 실행 process
            end user에게 서비스하게 되는 os 인 물리적이 서버에 셋팅
            24 시간 365일 실행되는 os에 설정해야 하는게 맞음

        3. 코드로 작업 적용 방식 
            1. py 파일 하나에  모든 코드 다 넣고 개발

            2. window인 py파일을 exe 파일로 변환 후에 외부 파일 실행 모듈 사용해서
                실행을 등록해서 서비스 하기도 함

            3. 참고 사이트
                https://hyrama.com/?p=579
                https://blog.naver.com/heennavi1004/222052800571


           
